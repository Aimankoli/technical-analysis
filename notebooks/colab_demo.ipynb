{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LLM-Augmented Technical Analysis - Colab Demo\n\nThis notebook runs the complete pipeline on Google Colab using:\n- **OpenAI GPT-4o-mini** for technical analysis\n- **FinBERT** for text embeddings\n- **ResNet18** fusion model for prediction\n\n**Dataset**: SPY 2015-2016 (reduced size for cost savings)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q yfinance pandas numpy scipy matplotlib mplfinance pillow scikit-learn torch torchvision transformers accelerate tqdm pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo (or upload files)\n",
    "!git clone https://github.com/YOUR_USERNAME/technical-analysis.git 2>/dev/null || echo \"Repo exists or using uploaded files\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Change to project directory\n",
    "if os.path.exists('technical-analysis'):\n",
    "    os.chdir('technical-analysis')\n",
    "\n",
    "sys.path.insert(0, '.')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up OpenAI API Key\n# Get your API key at: https://platform.openai.com/api-keys\n\nfrom google.colab import userdata\n\n# Option 1: Use Colab secrets (recommended - more secure)\n# Go to: Settings (gear icon) > Secrets > Add new secret\n# Name: OPENAI_API_KEY, Value: your-key-here\ntry:\n    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n    print(\"âœ“ API key loaded from Colab secrets\")\nexcept:\n    # Option 2: Set manually (less secure, visible in notebook)\n    os.environ['OPENAI_API_KEY'] = \"sk-your-api-key-here\"  # Replace with your key\n    print(\"âš  Set OPENAI_API_KEY manually - consider using Colab secrets instead\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories\n",
    "!mkdir -p data/raw data/samples data/images data/llm data/embeddings data/models configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Using reduced dataset to minimize LLM API calls:\n",
    "- Train: 2015 (~50 samples with stride=5)\n",
    "- Val: Jan-Jun 2016 (~25 samples)\n",
    "- Test: Jul-Dec 2016 (~25 samples)\n",
    "- Total: ~100 samples = ~100 Gemini API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import yaml\n\nconfig = {\n    # Data source\n    'ticker': 'SPY',\n    'start_date': '2014-01-01',\n    'end_date': '2017-12-31',\n    \n    # Dataset parameters\n    'lookback_days': 30,\n    'horizon_days': 5,\n    'image_size': 112,\n    'stride': 5,  # Larger stride = fewer samples = fewer API calls\n    'label_rule': 'forward_return_gt_0',\n    \n    # Splits - smaller dataset\n    'train_start': '2015-01-01',\n    'train_end': '2015-12-31',\n    'val_start': '2016-01-01',\n    'val_end': '2016-06-30',\n    'test_start': '2016-07-01',\n    'test_end': '2016-12-31',\n    \n    # Chart rendering\n    'include_volume': True,\n    'include_rsi_panel': True,\n    'overlay_sma20': True,\n    'overlay_bollinger': True,\n    'bb_window': 20,\n    'bb_k': 2,\n    'rsi_window': 14,\n    'hide_axes_labels': True,\n    'no_titles': True,\n    \n    # LLM - using OpenAI\n    'llm_enabled': True,\n    'llm_provider': 'openai',\n    'llm_model': 'gpt-4o-mini',\n    'temperature': 0,\n    'max_tokens': 220,\n    'cache_path': 'data/llm/spy_analysis.jsonl',\n    'llm_rate_limit_delay': 0.5,  # OpenAI has higher rate limits\n    \n    # FinBERT\n    'finbert_model': 'ProsusAI/finbert',\n    'embedding_dim': 768,\n    \n    # Training\n    'batch_size': 16,\n    'epochs': 15,\n    'lr': 1e-4,\n    'weight_decay': 1e-4,\n    'early_stop_patience': 5,\n    'seed': 42,\n    \n    # Fusion model\n    'use_numeric_features': True,\n    'use_text_embeddings': True,\n    'cnn_backbone': 'resnet18',\n    'dropout': 0.3,\n}\n\n# Save config\nwith open('configs/config.yaml', 'w') as f:\n    yaml.dump(config, f, default_flow_style=False)\n\nprint(\"Config saved. Key settings:\")\nprint(f\"  Stride: {config['stride']} (reduces samples by ~{config['stride']}x)\")\nprint(f\"  Train: {config['train_start']} to {config['train_end']}\")\nprint(f\"  Test: {config['test_start']} to {config['test_end']}\")\nprint(f\"  LLM: {config['llm_provider']} / {config['llm_model']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fetch Data & Compute Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_fetch import fetch_ohlcv, save_ohlcv\n",
    "from src.features import compute_all_indicators\n",
    "from pathlib import Path\n",
    "\n",
    "# Fetch OHLCV data\n",
    "df = fetch_ohlcv(config['ticker'], config['start_date'], config['end_date'])\n",
    "save_ohlcv(df, f\"data/raw/{config['ticker']}.csv\")\n",
    "\n",
    "# Compute indicators\n",
    "df_indicators = compute_all_indicators(df, config)\n",
    "df_indicators.to_csv(f\"data/raw/{config['ticker']}_indicators.csv\", index=False)\n",
    "\n",
    "print(f\"\\nData shape: {df_indicators.shape}\")\n",
    "df_indicators[['Date', 'Close', 'sma20', 'rsi14', 'bb_percent_b']].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Samples & Render Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.render_charts import build_samples, render_candlestick_chart\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Build samples\n",
    "samples_df = build_samples(df_indicators, config)\n",
    "samples_df.to_parquet(\"data/samples/samples.parquet\")\n",
    "\n",
    "print(f\"Total samples: {len(samples_df)}\")\n",
    "print(f\"\\nSplit distribution:\")\n",
    "for split in ['train', 'val', 'test']:\n",
    "    count = len(samples_df[samples_df['split'] == split])\n",
    "    print(f\"  {split}: {count}\")\n",
    "\n",
    "print(f\"\\nEstimated LLM API calls: {len(samples_df)}\")\n",
    "print(f\"Estimated time at 4s/call: {len(samples_df) * 4 / 60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render chart images\n",
    "print(\"Rendering chart images...\")\n",
    "for _, sample in tqdm(samples_df.iterrows(), total=len(samples_df)):\n",
    "    sample_id = sample['sample_id']\n",
    "    window_df = df_indicators.iloc[sample['start_idx']:sample['end_idx']+1].copy()\n",
    "    render_candlestick_chart(window_df, config, f\"data/images/{sample_id}.png\")\n",
    "\n",
    "print(f\"\\nRendered {len(samples_df)} chart images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display sample chart\n%matplotlib inline\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\n\n# Check if images were created\nimage_dir = \"data/images\"\nimage_files = [f for f in os.listdir(image_dir) if f.endswith('.png')] if os.path.exists(image_dir) else []\nprint(f\"Images created: {len(image_files)}\")\n\nif len(image_files) == 0:\n    print(\"No images found! Make sure the previous cell ran successfully.\")\nelse:\n    # Display a sample image\n    sample_id = samples_df.iloc[len(samples_df)//2]['sample_id']  # Middle sample\n    img_path = f\"data/images/{sample_id}.png\"\n    \n    if os.path.exists(img_path):\n        img = Image.open(img_path)\n        fig, ax = plt.subplots(figsize=(5, 5))\n        ax.imshow(img)\n        ax.axis('off')\n        ax.set_title(f\"Sample {sample_id} - No text, dates, or future info\")\n        plt.tight_layout()\n        plt.show()\n        print(f\"\\nImage size: {img.size}\")\n    else:\n        print(f\"Image not found at: {img_path}\")\n        print(f\"Available images: {image_files[:5]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. LLM Technical Analysis (OpenAI)\n\nThis step calls the OpenAI API for each sample. With ~100 samples and 0.5s delay, this takes ~1-2 minutes.\n\n**Note**: Results are cached in `data/llm/spy_analysis.jsonl`. If you rerun, it will skip already-processed samples."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test OpenAI API connection first\nfrom src.llm_analyze import call_openai\n\ntest_response = call_openai(\n    \"Return a JSON object with key 'status' and value 'ok'\",\n    \"You are a helpful assistant. Return only valid JSON.\",\n    model=config['llm_model']\n)\n\nif test_response:\n    print(f\"âœ“ OpenAI API working! Response: {test_response[:100]}...\")\nelse:\n    print(\"âœ— ERROR: OpenAI API not working. Check your OPENAI_API_KEY.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LLM analysis\n",
    "!python -m src.llm_analyze --config configs/config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check LLM outputs\n",
    "import json\n",
    "\n",
    "with open('data/llm/spy_analysis.jsonl', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f\"LLM analyses generated: {len(lines)}\")\n",
    "\n",
    "# Show a sample\n",
    "if lines:\n",
    "    sample = json.loads(lines[len(lines)//2])\n",
    "    print(f\"\\nSample analysis (id={sample['sample_id']}):\")\n",
    "    if sample.get('analysis_json'):\n",
    "        print(json.dumps(sample['analysis_json'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. FinBERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.finbert_embed --config configs/config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.train --config configs/config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.eval --config configs/config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.backtest --config configs/config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load all results\n",
    "with open('data/models/metrics.json') as f:\n",
    "    train_metrics = json.load(f)\n",
    "\n",
    "with open('data/models/eval_results.json') as f:\n",
    "    eval_results = json.load(f)\n",
    "\n",
    "with open('data/models/backtest_results.json') as f:\n",
    "    backtest = json.load(f)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nðŸ“Š Model Performance:\")\n",
    "print(f\"  Validation AUC: {train_metrics['best_val_auc']:.4f}\")\n",
    "print(f\"  Test AUC:       {eval_results['test']['auc']:.4f}\")\n",
    "print(f\"  Test Accuracy:  {eval_results['test']['accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ’° Backtest Results:\")\n",
    "strat = backtest['strategy']\n",
    "print(f\"  CAGR:           {strat['cagr']*100:.2f}%\")\n",
    "print(f\"  Sharpe Ratio:   {strat['sharpe']:.2f}\")\n",
    "print(f\"  Max Drawdown:   {strat['max_drawdown']*100:.2f}%\")\n",
    "print(f\"  Hit Rate:       {strat['hit_rate']*100:.2f}%\")\n",
    "print(f\"  Trades:         {strat['num_trades']}\")\n",
    "\n",
    "bh = backtest['buy_and_hold']\n",
    "print(f\"\\nðŸ“ˆ Buy & Hold Benchmark:\")\n",
    "print(f\"  Total Return:   {bh['total_return']*100:.2f}%\")\n",
    "\n",
    "excess = strat['total_return'] - bh['total_return']\n",
    "print(f\"\\nðŸŽ¯ Excess Return: {excess*100:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot equity curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "equity = np.array(backtest['equity_curve'])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(equity, label='Strategy')\n",
    "plt.axhline(y=10000, color='gray', linestyle='--', alpha=0.5, label='Initial Capital')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Equity ($)')\n",
    "plt.title('Backtest Equity Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}